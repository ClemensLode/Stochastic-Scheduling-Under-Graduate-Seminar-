\documentclass[a4paper,twoside]{report}
\usepackage{graphicx}
\usepackage{dsfont}
\input{Rahmen_Top}
\title{Dynamische Stochastische Optimierung}
\author{Clemens Lode}
\input{Rahmen_Mid}
\setcounter{chapter}{3}
\chapter{Stochastic scheduling}
\section{Einleitung}


Ich werde heute einen Vortrag ueber das sogenannte 'Stochastic Scheduling' halten. Im Speziellen moechte ich auf 4 Probleme aus diesem Bereich eingehen die mit der Abarbeitung von Jobs auf ein oder mehreren Prozessoren zu tun haben. Jeder Job benoetigt eine gewisse Zeit \(X_{j}\), die eine exponentiale Verteilung besitzt. %~~~

%...

\section{Fall: Ein Prozessor}

Zum Einstieg ein triviales Problem um mit den verwendeten Variablen und ein paar einfachen Gesetzmaessigkeiten im weiteren Verlauf besser zurechtzukommen. Wir haben n Jobs auszufuehren und einen einzelnen Prozessor zur Verfuegung, der immer nur einen Job gleichzeitig bearbeiten kann. Die Fragestellung ist nun, welche Reihenfolge der Jobs, im Folgenden Politik genannt, die Zeit minimiert um alle Jobs auszufuehren. 
Was ist die benoetigte Gesamtzeit unserer Jobs? Bei einem realistischen Beispiel waere das schwer zu sagen. Verbraucht z.B. ein Job sehr viel Speicher waehrend und nach der Bearbeitung waere wohl das beste diesen hinten hinzustellen, damit die anderen Jobs nicht auf der langsamen Festplatte auslagern muessen. Oeffnet ein Job mehrere Fenster die dann bis zum Ende aller anderen Jobs immer mitaktualisiert werden muessen, waere es wohl auch besser diesen vorne hinzustellen, der Prozessor koennte heisslaufen und die Leistung herunterschalten usw.
Wir machen es uns jedoch einfach und begrenzen uns auf den Fall, dass alle Jobs unabhaengig voneinander bearbeitet werden koennen.
%// Im weiteren gehen wir auch davon aus, dass zum Zeitpunkt 0 bereits 1 Job ... ~~~ UEBERLEGEN

%BILD! [      |  |      |   ]  

Im weiteren werde ich folgende Definitionen gebrauchen:

M: "Makespan" = benoetigte Gesamtzeit
Politik \begin{displaymath}\pi = (i_{1},i_{2},\ldots,i_{n})\end{displaymath} wobei \(i_{j}\) \(\neq\) \(i_{k}\) f"ur j \(\neq\) k und 0 < \(i_{j}\) \(\le\) n eine Jobnummer bezeichnet.
\(X_{j}\) ist eine exponential verteilte Zufallsvariable mit Parameter \begin{displaymath}\lambda = \frac{1}{u_{j}}\end{displaymath}.
%...

Die Gesamtzeit unserer Jobs ist dann natuerlich die Summe aller Zeiten, somit von der Reihenfolge unabh"angig und es ist, wie man es erwartet hatte, vollkommen egal, welche Politik man w"ahlt. Fuer die erwartete Gesamtzeit ergibt sich also:

\begin{displaymath} E(M) = E(X_{i_{1}})+E(X_{i_{2}})+\cdots+E(X_{i_{n}}) = \sum_{j=1}^{n} E(X_{j})\end{displaymath} <- unabh"angig von den \(i_{j}\)s

Mit dieser kleinen Einfuehrung koennen wir uns nun an ein schwierigeres Problem wagen, wir legen einen weiteren Prozessor hinzu:


\section{Fall: Zwei Prozessoren parallel}

Minimierung der zu erwartetenden Laufzeit - 2 Prozessoren parallelgeschaltet ~~

Problemdefinition:

Es sind eine Reihe Aufgaben gegeben, die unterschiedlich lange Zeit benoetigen. Ausserdem hat man ein System mit 2 Prozessoren zur Verfuegung, die beide gleich schnell arbeiten. Die Aufgaben sind stochastisch unabhaengig und exponential verteilt. Das Problem ist nun, eine Anordnung fuer die Abarbeitung der Aufgaben zu finden, so dass die insgesamt benoetigte Zeit minimal ist.

%<BILD/FOLIE! (Balken)>

Der erste Versuch eine Politik aufzustellen, waere also, die Aufgaben in 2 Reihen einzuteilen, die dann jeder Prozessor abarbeiten soll. Dies entspricht dem Problem, n Kugeln auf 2 nicht-unterscheidbare Urnen zu verteilen, wir haetten also bis zu (n = Aufgabenzahl) \begin{displaymath}n!+(n-1)!+\cdots+2!+1!\end{displaymath} verschiedene Politiken. 

%<BILD/FOLIE! (mehr Balken)>

Zwar koennte man auch mit diesem Modell mittels Verfahren von Stochastic Scheduling auf die einzelnen Aussagen und Saetze kommen, jedoch ist es bei Optimierungsaufgaben grundsaetzlich hilfreich, diese erst einmal zu vereinfachen, vor allem mit sog. 'Expertenwissen'.

Es ist klar, dass der Versuch auf Prozessor P1 n Aufgaben und auf Prozessor P2 0 Aufgaben zu verteilen fuer n>1 sicher nicht zum optimalen Ergebnis fuehren kann. So eine 'kaputte' Politik koennte man aber 'reparieren' in dem man, zu jedem Zeitpunkt in dem P2 keine Aufgabe zu bearbeiten hat, einfach die naechste Aufgabe aus unserer Politik zuweisen. Wenden wir diese Regel auf jede unserer Politiken an, folgt, dass eine Politik die alle n Aufgaben auf einen Prozessor legt, gleichwertig zu allen anderen Kombinationen der Verteilung der Aufgaben ist. Mit gleichwertig ist hier gemeint, dass damit eine optimale Politik gebildet werden kann, wenn auch ein paar moegliche, aber sicher nicht optimale Politiken wegfallen.

%<BEISPIEL>

Dies reduziert unseren Zahl unterschiedlicher Politiken auf n! und erleichtert die folgenden Beweise, da wir nur noch Politiken haben, die aus \emph{einer} anstatt zwei Reihen von Aufgaben bestehen. Wir koennen uns nun dem eigentlichen Problem widmen:

Die Frage ist nun zuerst, ob durch unsere Regel das Problem nicht schon geloest wurde. Wenn wir immer eine Aufgabe auf den Prozessor legen, der nicht beschaeftigt ist, gleichen wir bei jedem Schritt beide Balken aus. Erreichen wir dadurch Gleichgewicht? Nein. Dies wuerde der Fall sein, wenn alle Aufgaben gleiche Zeit benoetigten und eine gerade Anzahl Aufgaben vorliegt. Legen wir aber beispielsweise die Aufgaben mit aufsteigenden Werten auf die Prozessoren, schwankt der Unterschied zwischen beiden bei jedem Schritt mehr, 
	
\begin{displaymath}\sum_{2i+1}^{n} a_{i} = \sum_{2i+2}^{n} a_{i}, \sum_{2i+1}^{n} b_{i} = \sum_{2i}^{n} b_i, a_(2i+2) = 2i+1, b_(2i) = 2i\end{displaymath}
%\begin{displaymath}\Rightarrow \sum_{i=0}^{n} a_{i} - \sum_{i=0}^{n} b_{i} = \sum_(2i+1) a_i - SUM_(2i+1) b_i + SUM_(2i+2) a_i - SUM_(2i+2) b_i = SUM_(2i+1) (2i+1 - 2i) + SUM_(2i+2) (2i+1 - 2i+2) = (2i+1) + (-2i+2) = 1.\end{displaymath} 
%TODO!!!
%!!!!!!!!!!!!!!!!!!
\begin{tabular}{|r|ccc|}
\hline
Schritt & Idle-Zeit & 1. Prozessor & 2. Prozessor \\
\hline \\
0 & 1 & 1 & 0 \\
1 & 1 & 1 & 2 \\
2 & 2 & 4 & 2 \\
3 & 2 & 4 & 6 \\
4 & 3 & 9 & 6 \\
5 & 3 & 9 & 12 \\
6 & 4 & 16 & 12 \\
7 & 4 & 16 & 20 \\
... & ... & ... & ... \\
\hline
\end{tabular}
%\caption{Idlezeit bei aufsteigender Sortierung}

Dass dies auch wirklich ein Problem darstellt, die Gesamtzeit also von der Reihenfolge abhaengt, macht dieses Beispiel schnell klar:

Angenommen wir haetten 5 Aufgaben mit den Zeiten 10, 8, 12, 5 und 1. Legen wir sie nun auf die 2 Prozessoren:

%P1 |||||||||#||||##
%P2 |||||||#|||||||||||#

Man sieht also, dass mit dieser Reihenfolge ein Prozessor 4 Zeiteinheiten unbeschaeftigt ist. 
Auch wird klar, dass egal wie wir es anstellen, die untere Schranke fuer die benoetigte Zeit (10 + 8 + 12 + 5 + 1) / 2 ist, wenn also beide Prozessoren bis zum Ende voll beschaeftigt sind.

Investiert man in die Aufgabe etwas Denkarbeit wird auch klar, dass wohl die optimale Politik waere, wenn wir grundsaetzlich mit den laengsten Aufgaben beginnen und am Schluss die kuerzesten Aufgaben legen. Warum? Weil wir bei jedem Schritt die Aufgabe grundsaetzlich nur auf den Prozessor legen, der gerade frei ist. Wir versuchen also bereits dadurch die beiden Balken auszugleichen. 


Das ganze nochmal formal aufgeschrieben und bewiesen, und schon sind wir mit dem ersten Teil fertig:


Seien 2 identische Prozessoren gegeben um n Aufgaben abzuarbeiten. Beide laufen unabhaengig, koennen also jede Aufgabe zu jeder Zeit in einer beliebigen Reihenfolge bearbeiten. Startzeit ist t=0.
Sei \(X_{j}\) die Zeit um Aufgabe j zu bearbeiten und sei diese exponentialverteilt mit \begin{displaymath}\lambda = u_j, j=1,\ldots,n\end{displaymath}.
Eine Politik ist eine Permutation \begin{displaymath}i_{1},\ldots,i_{n} = 1,2,\ldots,n\end{displaymath} wie die Aufgaben der Reihe nach angeordnet sind. Die gesamte Zeit bis alle Aufgaben erf"ullt sind wird 'makespan' genannt und das Ziel ist, die makespan zu minimieren. TODO SCHON OBEN GESCHRIEBEN

Fuer jede Politik \begin{displaymath}\pi = (0,i_{1},\ldots, i_{n})\end{displaymath} ist jeweils M  die Makespan und D die Zeit die einer der Prozessoren stillsteht. Somit ist zum Zeitpunkt M-D einer der Prozessoren fertig und keine weiteren Auftraege mehr in der Warteschlange und M + M - D die Summe aller Zeiten, also \begin{displaymath}\sum_{j=0}^n X_{j}\end{displaymath}

%#TODO JOB 0 IST ERSTER JOB IN DER SCHLANGE ~~~

%<BILD>

Also gilt:

\begin{displaymath}2M - D - \sum_{j=0}^{n} X_{j} = 0\end{displaymath}

und somit fuer jede Strategie pi:

\begin{displaymath}E_{\pi}(2M-D-\sum_{j=0}^{n} X_{j}) = E_{\pi}(0)\end{displaymath}

%<\Rightarrow

\begin{displaymath}2* E_{\pi} (M) = E_{\pi} (D) + \sum_{j=0}^{n} E_{\pi} (X_{j})\end{displaymath}

Da nach Voraussetzung \(X_{j}\) von der Reihenfolge, also der Politik pi nicht abhaengt, ist \(\sum_{j=0}^{n} E(X_{j})\) eine Konstante fuer jede Politik pi (fuer die gleichen \(X_{j}\) und wir setzen \begin{displaymath}c = \sum_{j=0}^{n} E(X_{j})\end{displaymath}:

\begin{displaymath}E_{\pi}(M) = (E_{\pi}(D) + c)/2\end{displaymath}

Also ergibt sich nun auch formal, dass durch Minimierung der erwarteten Differenz D in der einer der Proezessoren unbeschaeftigt ist, auch die Makespan minimiert wird.


Als naechstes zeige ich, dass eine Politik die in aufsteigender Reihenfolge der \(u_{i}\)s (also der ~~~ aufsteigend Zeiten) angeordnet ist, optimal ist.
Dies wird wie folgt induktiv bewiesen:

Behauptung: Sei eine Politik \begin{displaymath}\pi = (0,2,1,3,\ldots,n)\end{displaymath} und eine Politik \begin{displaymath}\pi` = (0, 1,2,3,\ldots,n)\end{displaymath} gegeben. Ist \begin{displaymath}u_{1} = min k \ge 1 u_{k}\end{displaymath} dann gilt \begin{displaymath}E_{\pi`}(D)\le = E_{\pi}(D)\end{displaymath}.

Die Idee ist also, eine einzelne Transposition durchzufuehren und dann zu beweisen, dass sich dadurch \(E_{\pi}\)(D) erhoeht.

Beweis:

Seien \(P_{j}\) und \(P`_{j}\), j \(\ge\) 0 die zu Pi und Pi` gehoerigen Wahrscheinlichkeiten, dass die letzte von den Prozessoren abgeschlossene Aufgabe Job j ist, also z.B. Prozessor A fertig ist und Prozessor B noch an Aufgabe j arbeitet und keine weiteren Aufgaben mehr in der Warteschlange/Politik liegen. %~~~

Klar ist \begin{displaymath}P_{0} = P`_{0} = P(X_{0} > \sum_{j=1}^{n} X_{j})\end{displaymath}, dass also der erste Job der Wahrscheinlichkeit entspricht, dass \(X_{0}\) groesser als alle anderen zusammengenommen ~~ ergibt, da \(P_{0}\) ja jeweils der erste Job in beiden Politiken ist.

Wir wollen nun durch Induktion zeigen, dass \begin{displaymath}P`_{1} \le P_{1}, P`_{j} \ge P_{j}\end{displaymath} f"ur \begin{displaymath}j=2,\ldots,n\end{displaymath}.

Induktionsanfang: Fuer n=1 faellt \begin{displaymath}P`_{j} \ge P_{j}\end{displaymath} fuer j=2,\ldots,n sowieso weg, da n<2 und \begin{displaymath}P`_{1} \le P_{1}\end{displaymath} gilt nach der Voraussetzung \(u_{1}\) = min \(u_{k}\), da Pi an der Stelle 1 \(u_{1}\) stehen hat und Pi` dagegen \(u_{2}\).

Induktionsvoraussetzung: Es gelte also die Behauptung fuer den Fall, dass nur n-1 jobs (zusaetzlich zu job 0) vorliegen.

Induktionsschluss: Seien nun n jobs zu erledigen und seien \(P*_{j}\) und \(P`*_{j}\), wieder zugehoerig zu Pi und Pi`, j=1,\ldots,n-1 die Wahrscheinlichkeiten, dass Job j der letzte der Jobs 0,\ldots,n-1.

Exponential and the "Lack of Memory" property 	Here the "Lack of Memory" property of the exponential is used. See page 198-199 and equation 4-12. Essentially, this property says that the P(X>t) does not depend on how much time has already elapsed. For example, if the average life of a light bulb is 2 years (exponentially distributed), then the probability that the bulb will last 1 more year does not depend on how old the bulb already is.

%~~ Dadurch, und das bei beiden Politiken der letzte Job n ist, erhalten wir:

\begin{displaymath}P_j = P*_j u_n / (u_n + u_j) und P`_j = P`*_j u_n / (u_n + u_j), j = 1...n-1\end{displaymath}


Woher kommt dieses \begin{displaymath}\frac{u_{n}}{u_{n}+u_{j}}\end{displaymath}?
%Das frage ich mich auch :-(

Also, was ist \(u_{n}\)? u war die Lambda der Exponentialverteilung, also \begin{displaymath}0 \le u \le 1\end{displaymath} und \begin{displaymath}X_{j} = F(t) = 1 - e^{-u_{j}t}\end{displaymath}.
Aus 0 \(\le\) u folgt, dass der Quotient <1 ist, die Wahrscheinlichkeit also kleiner ist, als \(P*_{j}\). 
Nun, wie haben wir \(P*_{j}\) definiert? \(P*_{j}\) war die Wahrscheinlichkeit, dass j der letzte Job ist, wenn der n-te Job weggelassen wird, also im Fall mit (n-1) Jobs. Legen wir nun einfach noch den n-ten Job hinzu, ergeben sich logischerweise 2 F"alle: j bleibt der letzte Job oder n wird der letzte Job. Da dies auf den ersten Blick nicht einleuchtend ist, hier wieder ein Bild:

%|||||||||| (n-4) ||||||||| (n-2) ||| (n-1) ||||||||||| (n)
%|||||||||||||||||| (n-3) ||||||||||||||||||| (j = n-2)

\(P_{j}\) war definiert als \begin{displaymath}P(X_{j} > \sum_{i=1}^{n} X_{i} - X_{j}\end{displaymath}
\(P*_{j}\) war definiert als \begin{displaymath}P(X_{j} > [\sum_{i=1}^{n-1} X_{i}] - X_{j})) = P(X_{j} > [\sum_{i=1}^{n-1} X_{i}] - X_{j} + X_{n}) * \frac{u_{n} + u_{j}}{u_{n}}\end{displaymath}

%HIER NOCHMAL NACHFRAGEN!

\begin{displaymath}\Rightarrow P_{j} =  P*_{j} * \frac{u_{n}}{u_{n} + u_{j}}\end{displaymath}

und \begin{displaymath}P`_{j} = P`*_{j} * \frac{u_{n}}{u_{n} + u_{j}}\end{displaymath}

\begin{displaymath}\Rightarrow P`_{1} \le P_{1}, P`_{j} \ge P_{j}, j=2,\ldots,n-1\end{displaymath}


Kennen wir alle Wahrscheinlichkeiten \(P_{0}\) bis \(P_{n-1}\) kennen wir auch \(P_{n}\), da egal welche Politik wir waehlen AUF JEDEN FALL eine Politik die letzte ist.

Wir benutzen einfach die Gegenwahrscheinlichkeit:

\begin{displaymath}P_{n} = 1 - \sum_{j=0}^{n} P_{j}\end{displaymath}

bzw. \begin{displaymath}P`_n - P_n =  \sum(P`_j - P_j) = \sum((P*_{j} - P`*_{j}) * \frac{u_{n}}{u_{n} + u_{j}} =\end{displaymath}
\begin{displaymath}(P`*_{1} - P*_{1}) \frac{u_{1}}{u_{1} + u_{n}} + \sum_{j=2}^{n-1} (P`*_{j} - P*_{j}) \frac{u_{j}}{u_{j}+u_{n}} \end{displaymath}
%~~` ja ok
%~~

Wir haben jetzt also per Induktion gezeigt, dass der Erwartungswert der Zeit in der ein Prozessor still steht bei Politik (0,2,1,3,\ldots,n) mindestens so gross ist wie bei (0,1,2,3,\ldots,n), falls \begin{displaymath}u_{1} = min k u_{k}\end{displaymath}

Mit dieser Grundlage koennen wir nun zusaetzlich zeigen, dass (0,1,2,...n) die optimale Politik ist um den Erwartungswert fuer D zu minimieren.

Also:
Sei wieder \begin{displaymath}u_{1} \le u_{2} \le \ldots \le u_{n}\end{displaymath} (*)

Nehmen wir also eine neue Politik her, die nicht mit (0,1,...) beginnt, dann koennen wir mit dem bewiesenen Satz 3.1~~~~ direkt folgern, dass diese neue Politik mehr Zeit benoetigt als die alte. Wenn wir jetzt noch zeigen koennten, dass wenn Job 1 an einer beliebigen anderen Stelle k steht, die Politik besser waere bei der die Stellen k und k-1 vertauscht sind, haetten wir die Behauptung bewiesen:

Seien also P und P` zwei Politiken mit der selben Reihenfolge der Jobs, wobei bei P an Stelle k eine 1 steht und bei P` an Stelle k-1 eine 1 steht und \(i_{k}\) von P` = \(i_{k-1}\) von P ist.

\begin{displaymath}P  = (0,i_{1},i_{2},\ldots,i_{k-2},i_{k-1},i_{k} = 1,i_{k+1},\ldots,i_{n})\end{displaymath}
\begin{displaymath}P` = (0, i_{1}, i_{2},\ldots, i_{k-2}, i_{k-1} = 1, i_{k}, i_{k+1},\ldots, i_{n})\end{displaymath}

Wieder nach der Eigenschaft des \textsc{lack of memory} haben die Jobs 0 bis \(i_{k-2}\) keinen Einfluss auf die Nachfolgenden, wie auch \(i_{k-1}\) und \(i_{k}\) keinen Einfluss auf die Zeiten der nachfolgenden Jobs haben. Somit kann man diese zu einem neuen Job 0 zusammenfassen und es ergibt sich:

\begin{displaymath}P = (0, i_{k-1}, i_{k} = 1,\ldots), P`= (0, i_{k-1} = 1, i_{k},\ldots)\end{displaymath}
bzw.
\begin{displaymath}P = (0, x, 1,\ldots), P`= (0, 1, x,\ldots)\end{displaymath}

In 3.1 ~~~ haben wir nur (0,2,1,3,\ldots) mit (0,1,2,3,\ldots) verglichen. Es ist aber nicht entscheidend, ob beim Beweis die zweite Stelle 2 oder irgend eine andere Zahl j>1 ist, sondern nur, dass \begin{displaymath}u_{j} \le u_{1}\end{displaymath} gilt. Da hier x > 1, also j > 1 ist, folgt nach Voraussetzung (*), dass \begin{displaymath}u_{1} \le u_{j}\end{displaymath} und somit ist Lemma 3.1 anwendbar.

Das Spielchen kann nun mit 2, 3 usw. wiederholt werden, bis alle Jobs in aufsteigender Reihenfolge da stehen woraus die Behauptung folgt und bewiesen ist, dass eine aufsteigende Jobanordnung die Aufgabenstellung loest. Zu Betonen ist hierbei, dass man dies, wie gesagt, in aufsteigender Reihenfolge durchfuehrt, da ein solches Vertauschen eines niederwertigen durch ein hoeherwertigen nicht grundsaetzlich von Vorteil ist, denn es muss nicht nur \begin{displaymath}u_{j} \le u_{k}\end{displaymath} sein, \(u_{j}\) muss auch das kleinste Element, abgesehen von \(u_{j-1}\), \(u_{j-2}\) etc. die ja schon nach vorne geschoben wurden, aller Jobs sein, also \begin{displaymath}u_{j} \le min u_{j+1},\ldots,u_{n}\end{displaymath}%~~~~) sein.~~~~ //TODO

Das waere es eigentlich fast schon zum zweiten Punkt dieses Vortrags. Ich moechte jedoch noch einmal kurz auf die anfangs erwaehnten Politiken eingehen, die man durch geschicktes Waehlen der Form der Politik und der Einfuegeregel wegfallen hat lassen.

\newpage
\section{Ein Prozessor mit begrenzter Zeit}

Mit diesem Problembeispiel moechte ich auf einen weiteren Aspekt des Stochastic Scheduling eingehen. Wir haben wie vorher auch n Jobs die ausgefuehrt werden sollen, haben diesmal aber eine feste Zeitbegrenzung t, die wahrscheinlich dazu f"uhrt, dass nicht alle Jobs ausgefuehrt werden koennen.
Der Unterschied zum ersten Beispiel ist nun aber, dass wir jedem Job i eine Prioritaet \(R_{i}\) zuordnen und die Qualitaet einer Politik nicht wie vorher ueber die Zeit sondern ueber die Summe der Priorit"aten bestimmen, es soll also \begin{displaymath}\sum_{i=0}^{n}R_{i}s_{i}\end{displaymath} maximiert werden, wobei \(s_{i}\) 1 ist, wenn der Job ausgefuehrt werden konnte und 0 wenn nicht.
Auch hier gilt wieder die 'lack-of-memory' Eigenschaft, frueher ausgefuehrte Jobs haben also keine Wirkung auf die benoetigte Zeit von nachfolgenden Jobs.

Das \(s_{i}\) kann man natuerlich auch als Wahrscheinlichkeit schreiben, wenn man den Erwartungswert der Qualitaet der Politik haben will: ~~~

....


"Lack of memory" means the probability of failure in a specific time interval is the same regardless of the starting point of that time interval %TODO
"gedaechtnislos"

Interpretation: Ein Objekt lebt bereits mehr als x
Zeiteinheiten. Dann ist die bedingte Wkt. dafur, ¨
dass es innerhalb der n¨  achsten x Zeiteinheiten
ausf¨ lt, gleich der unbedingten Wkt., dass es uber-
    al                                         ¨
haupt h¨ ochstens x Zeiteinheiten lebt. Mit anderen
Worten, das Objekt ist stets (so gut wie) neu" .
                              "






\newpage
\section{Zwei Prozessoren mit begrenzter Zeit}

Im letzten Teil kombinieren wir nun die vorherigen 2 Problemstellungen. Wir haben 2 Prozessoren, eine feste Zeit t und fuer jeden Job eine Prioritaet \(R_{j}\).
Nach 2.1 ist der Gesamtgewinn ~~ unter einer Politik \(\pi\):
\(E_{\pi}\)(Gesamtgewinn abh"angig von t) = \begin{displaymath}\sum_{j=0}^{n} u_{j}R_{j}E_{\pi}\end{displaymath}(Verarbeitungszeit von Job j nach t ~~)

Es sieht nun so aus, als ob wir hier wie in 2.1, der Fall bei dem wir nur ein Prozessor zur Verfuegung hatten, die optimale Politik eine absteigende Reihenfolge der \(u_{j}\)\(R_{j}\) ist, also wir Jobs um so weiter hinten positionieren je groesser die erwartete benoetigte Zeit und je kleiner die Prioritaet ist.

Dies ist aber nicht der Fall wie ein kleines Gegenbeispiel zeigt:

\(u_{j}\)\(R_{j}\) == 1 f"ur alle j und \begin{displaymath}u_{1} < u_{2} < \cdots < u_{n}\end{displaymath}.

Es wuerde folgen, dass, da ja alle \(u_{j}\)\(R_{j}\) gleich gross sind, jede beliebige Reihenfolge optimal ist. Dies stimmt auch im Fall von einem Prozessor, bei zwei Prozessoren tritt jedoch wie anfangs besprochen eine immer groessere Idle Zeit auf, wenn wir die Jobs absteigend nach den \(u_{j}\)s (bzw. aufsteigend nach den benoetigten Zeiten) sortieren.

Wir koennen jedoch zeigen, dass wie im Fall ohne Prioritaeten eine aufsteigende Reihenfolge optimal ist, wenn wir gewisse Bedingungen voraussetzen.

Behauptung:
Sei 
\begin{displaymath}u_{1} \le u_{2} \le \cdots \le u_{n}\end{displaymath} 
und 
\begin{displaymath}u_{1}R_{1} \ge u_{2}R_{2} \ge \cdots \ge u_{n}R_{n} \ge 0\end{displaymath}
dann ist die Reihenfolge (1, 2, \ldots, n) optimal und maximiert den zu erwartenden Gesamtgewinn fuer Zeit t > 0.

Beweis:




\newpage
\section{Weitere verwandte Probleme und Anwendungen}
Schluss:

Man kann sich eine Vielzahl von weiteren Problemen der Art denken:
2 Prozessoren hintereinander, 2x 2 Prozessoren hintereinander parallelgeschaltet, Aenderung der Prozessorgeschwindigkeit waehrend des Verlaufs, Ausfall von Prozessoren usw. usw.

\newpage
\section{Zusammenfassung}

\begin{thebibliography}{99}
\bibitem{Ross} {\sc Sheldon M. Ross:}  \textit{Introduction to stochastic dynamic programming - Probabilityand mathematical statistics}, First edition, Academic Press, Inc., 1983
\end{thebibliography}
\end{document}


