\documentclass[a4paper,twoside]{report}
\usepackage{graphicx}
\usepackage{dsfont}
\input{Rahmen_Top}
\title{4. Vortrag: Stochastic Scheduling (Sect. 6.2 und 6.3)}
\author{Clemens Lode}
\input{Rahmen_Mid}
\setcounter{chapter}{3}
\chapter{Stochastic scheduling}
\section{Einleitung}


Ich werde heute einen Vortrag "uber das sogenannte 'Stochastic Scheduling' halten. Im Speziellen m"ochte ich auf 4 Probleme aus diesem Bereich eingehen die mit der Abarbeitung von Jobs auf ein oder mehreren Prozessoren zu tun haben. Jeder Job ben"otigt eine gewisse Zeit \(X_{j}\), die eine exponentiale Verteilung besitzt. %~~~

%...

\section{Fall: Ein Prozessor}

Zum Einstieg ein triviales Problem um mit den verwendeten Variablen und ein paar einfachen Gesetzm"assigkeiten im weiteren Verlauf besser zurechtzukommen. 
Das Problem ist, dass wir n Jobs auszuf"uhren haben. Ein Job ist dabei eine beliebige Aufgabe die ein Prozessor berechnen kann, der eine bestimmte Zeit braucht, wobei die Jobs nicht gleich sein muessen.
Au"serdem haben wir im ersten Fall einen einzelnen Prozessor zur Verf"ugung, der immer nur einen Job gleichzeitig bearbeiten kann. Die Fragestellung ist nun, welche Reihenfolge der Jobs, im Folgenden Politik genannt, die Zeit um alle Jobs auszuf"uhren minimiert.

Was ist die ben"otigte Gesamtzeit unserer Jobs? Bei einem realistischen Beispiel w"are das schwer zu sagen. Verbraucht z.B. ein Job w"ahrend und nach der Bearbeitung sehr viel Speicher w"are wohl das Beste diesen hinten hinzustellen, damit die anderen Jobs nicht z.B. auf der langsamen Festplatte ausgelagert werden m"ussen. Die Zeit die ein einzelner Job ben"otigt w"are also vom bisherigen Verlauf abh"angig.

Wir machen es uns jedoch einfach und begrenzen uns auf den Fall, dass alle Jobs unabh"angig voneinander bearbeitet werden k"onnen. Dies bedeutet, die einzelnen Jobs sind \emph{ged"achtnislos}, d.h. in diesem Fall, dass zu jedem Zeitpunkt t auf Basis der bisherigen Ereignisse die bedingte Wahrscheinlichkeit fuer jeden neuen Job eine bestimmte Zeit zu brauchen genau so gro"s ist, als w"are dies der erste Job in der Auftragsliste.

%BILD! [      |  |      |   ]  

Im weiteren werde ich folgende Definitionen und Voraussetzungen gebrauchen:
\begin{itemize}
\item M: "makespan" = ben"otigte Gesamtzeit bis alle Jobs abgearbeitet sind
\item Politik \begin{displaymath}\pi = (i_{1},i_{2},\ldots,i_{n})\end{displaymath} wobei \(i_{j}\) \(\neq\) \(i_{k}\) f"ur \(j \neq k\) und \(0<i_{j}\le n\) eine Jobnummer bezeichnet.
\item \(X_{j}\) ist eine exponential verteilte Zufallsvariable mit Parameter \begin{displaymath}\lambda = \frac{1}{u_{j}}\end{displaymath}.
%...
\end{itemize}
Im Weiteren gehen wir au"serdem davon aus, dass zum Zeitpunkt 0 bereits ein Job 0 auf dem Prozessor liegt.


Die Gesamtzeit unserer Jobs ist dann nat"urlich die Summe aller Zeiten und somit von der Reihenfolge unabh"angig. Also ist es, wie man es erwartet h"atte, vollkommen egal, welche Politik man w"ahlt. F"ur die erwartete Gesamtzeit ergibt sich also:

\begin{displaymath} E(M) = E(X_{i_{1}})+E(X_{i_{2}})+\cdots+E(X_{i_{n}}) = \sum_{j=1}^{n} E(X_{j})\end{displaymath} \(\leftarrow\) unabh"angig von den \(i_{j}\)s

Mit dieser kleinen Einf"uhrung k"onnen wir uns nun an ein schwierigeres Problem wagen, wir legen im zweiten Teil einen weiteren Prozessor hinzu.

\newpage\section{Fall: Zwei Prozessoren parallel}

Problemdefinition:

Es sind wieder eine Reihe von Jobs gegeben, die unterschiedlich lange Zeit ben"otigen. Au"serdem hat man ein System mit diesmal 2 Prozessoren zur Verf"ugung, die beide gleich schnell arbeiten. Die einzelnen Jobs sind stochastisch unabh"angig und exponential verteilt. Das Problem ist nun wieder, eine Politik f"ur die Abarbeitung der Aufgaben zu finden, so dass die insgesamt ben"otigte Zeit minimal ist.

%<BILD/FOLIE! (Balken)>


Ein erster Ansatz w"are, die Politiken so aufzubauen, dass wir jedem Job j einen Zeitpunkt k und einen Prozessor l zuordnen. Zwar sind damit alle m"oglichen Kombinationen abgedeckt, jedoch lassen wir eine wertvolle Information unter den Tisch fallen: Wir m"ussen nicht schon vor Beginn des Einf"ugens alle Positionen unserer Jobs kennen. Was wir nur wissen m"ussen ist, welchen Job wir einf"ugen, nachdem auf einem Prozessor wieder ein Job abgearbeitet wurde.
Somit haben wir die L"ucken geschlo"sen, die ja vor allem deshalb auftreten, weil die Zeiten \(X_{j}\) exponential verteilt und nicht konstant sind.

%<Bild

Der zweite Versuch eine Politik aufzustellen, w"are also, die Aufgaben in 2 Reihen einzuteilen, die dann jeder Prozessor abarbeiten soll. Dies entspricht dem Problem, n Kugeln auf 2 nicht-unterscheidbare Urnen zu verteilen, wir h"atten also bis zu (n = Aufgabenzahl) \begin{displaymath}n!+(n-1)!+\cdots+2!+1!\end{displaymath} verschiedene Politiken. 

%<BILD/FOLIE! (mehr Balken)>

Zwar k"onnte man auch mit diesem Modell mittels Verfahren von Stochastic Scheduling auf die einzelnen Aussagen und S"atze kommen, jedoch ist es bei Optimierungsaufgaben grunds"atzlich hilfreich, diese so weit wie moeglich zu vereinfachen, vor allem mit sogenanntem 'Expertenwissen'. Dabei ist nat"urlich immer darauf zu achten, dass der optimale Fall mit der Politik noch herzustellen ist.
% evtl Paragraph teilweise hoch

Es ist klar, dass der Versuch auf Prozessor \(P_{1}\) n Jobs und auf Prozessor \(P_{2}\) 0 Jobs zu verteilen f"ur \(n>1\) sicher nicht zum optimalen Ergebnis f"uhren kann. So eine 'kaputte' Politik k"onnte man aber 'reparieren' in dem man, zu jedem Zeitpunkt bei dem P2 keine Aufgabe zu bearbeiten hat, einfach die n"achste Aufgabe aus unserer Politik zuweisen. Wenden wir diese Regel auf jede unserer Politiken an, folgt, dass eine Politik die alle n Aufgaben auf einen Prozessor legt, \emph{gleichwertig} zu allen anderen Kombinationen der Verteilung der Aufgaben ist. Mit gleichwertig ist hier gemeint, dass damit eine optimale Politik gebildet werden kann, wenn auch ein paar m"ogliche, aber sicher nicht optimale Politiken wegfallen.

%<BEISPIEL>


Dies reduziert unseren Zahl unterschiedlicher Politiken auf n! und erleichtert die folgenden Beweise, da wir nur noch Politiken haben, die aus \emph{einer} anstatt zwei Reihen von Aufgaben bestehen. Wir k"onnen uns nun dem eigentlichen Problem widmen:

Die Frage ist nun zuerst, ob durch unsere Regel das Problem nicht schon gel"ost wurde. Wenn wir immer eine Aufgabe auf den Prozessor legen, der nicht besch"aftigt ist, gleichen wir bei jedem Schritt beide Balken aus. Erreichen wir dadurch das optimale Gleichgewicht? Nein. Dies w"urde der Fall sein, wenn alle Aufgaben gleiche Zeit ben"otigten und eine gerade Anzahl Aufgaben vorliegt. Legen wir aber beispielsweise die Aufgaben mit aufsteigenden Werten auf die Prozessoren, schwankt der Unterschied zwischen beiden bei jedem Schritt mehr, 
	
\begin{displaymath}\sum_{2i+1}^{n} a_{i} = \sum_{2i+2}^{n} a_{i}, \sum_{2i+1}^{n} b_{i} = \sum_{2i}^{n} b_{i}, a_{2i+2} = 2i+1, b_{2i} = 2i\end{displaymath}
%\begin{displaymath}\Rightarrow \sum_{i=0}^{n} a_{i} - \sum_{i=0}^{n} b_{i} = \sum_(2i+1) a_i - SUM_(2i+1) b_i + SUM_(2i+2) a_i - SUM_(2i+2) b_i = SUM_(2i+1) (2i+1 - 2i) + SUM_(2i+2) (2i+1 - 2i+2) = (2i+1) + (-2i+2) = 1.\end{displaymath} 
%TODO!!!
%!!!!!!!!!!!!!!!!!!
\begin{tabular}{|r|ccc|}
\hline
Schritt & Idle-Zeit & 1. Prozessor & 2. Prozessor \\
\hline \\
0 & 1 & 1 & 0 \\
1 & 1 & 1 & 2 \\
2 & 2 & 4 & 2 \\
3 & 2 & 4 & 6 \\
4 & 3 & 9 & 6 \\
5 & 3 & 9 & 12 \\
6 & 4 & 16 & 12 \\
7 & 4 & 16 & 20 \\
... & ... & ... & ... \\
\hline
\end{tabular}
%\caption{Idlezeit bei aufsteigender Sortierung}

%Dass dies auch wirklich ein Problem darstellt, die Gesamtzeit also von der Reihenfolge abh"angt, macht dieses Beispiel schnell klar:
%Angenommen wir h"atten 5 Aufgaben mit den Zeiten 10, 8, 12, 5 und 1. Legen wir sie nun auf die 2 Prozessoren:

%P1 |||||||||#||||##
%P2 |||||||#|||||||||||#

%Man sieht also, dass mit dieser Reihenfolge ein Prozessor 4 Zeiteinheiten unbesch"aftigt ist. 
%Auch wird klar, dass egal wie wir es anstellen, die untere Schranke f"ur die ben"otigte Zeit (10 + 8 + 12 + 5 + 1) / 2 ist, wenn also beide Prozessoren bis zum Ende voll besch"aftigt sind.

Investiert man in die Aufgabe etwas Denkarbeit wird auch klar, dass wohl die optimale Politik w"are, wenn wir grunds"atzlich mit den l"angsten Jobs beginnen und am Schluss die k"urzesten Jobs legen. Warum? Weil wir bei jedem Schritt die Aufgabe grunds"atzlich nur auf den Prozessor legen, der gerade frei ist. Wir versuchen also bereits dadurch die beiden Balken auszugleichen. Je sp"ater wir eine l"angere Aufgabe auf einen Prozessor legen, desto unwahrscheinlicher k"onnen wir diesen mit den restlichen Jobs wieder ausgleichen.

Das ganze nochmal formal aufgeschrieben und bewiesen, und schon sind wir mit dem ersten Teil fertig:

Seien 2 identische Prozessoren gegeben um n Aufgaben abzuarbeiten. Beide laufen unabh"angig, k"onnen also jede Aufgabe zu jeder Zeit in einer beliebigen Reihenfolge bearbeiten. Startzeit ist t = 0.
Sei \(X_{j}\) die Zeit um Aufgabe j zu bearbeiten und sei diese exponentialverteilt mit \begin{displaymath}\lambda = u_j, j=1,\ldots,n\end{displaymath}.
Eine Politik ist eine Permutation \begin{displaymath}i_{1},\ldots,i_{n} = 1,2,\ldots,n\end{displaymath} wie die Aufgaben der Reihe nach angeordnet sind. Die gesamte Zeit bis alle Aufgaben erf"ullt sind wird 'makespan' genannt und das Ziel ist, die makespan zu minimieren. TODO SCHON OBEN GESCHRIEBEN

F"ur jede Politik \begin{displaymath}\pi = (0,i_{1},\ldots, i_{n})\end{displaymath} ist jeweils M  die Makespan und D die Zeit die einer der Prozessoren stillsteht. Somit ist zum Zeitpunkt M - D einer der Prozessoren fertig und keine weiteren Auftr"age mehr in der Warteschlange und M + M - D die Summe aller Zeiten, also \begin{displaymath}\sum_{j=0}^n X_{j}\end{displaymath}

%#TODO JOB 0 IST ERSTER JOB IN DER SCHLANGE ~~~
%TODO EQUATIONS + VERWEISE IM TEXT
%<BILD>

Also gilt:

\begin{displaymath}2M - D - \sum_{j=0}^{n} X_{j} = 0\end{displaymath}

und somit f"ur jede Strategie \(pi\):

\begin{displaymath}E_{\pi}(2M-D-\sum_{j=0}^{n} X_{j}) = E_{\pi}(0)\end{displaymath}

%<\Rightarrow

\begin{displaymath}2* E_{\pi} (M) = E_{\pi} (D) + \sum_{j=0}^{n} E_{\pi} (X_{j})\end{displaymath}

Da nach Voraussetzung \(X_{j}\) von der Reihenfolge, also der Politik \(\pi\) nicht abh"angt, ist \(\sum_{j=0}^{n} E(X_{j})\) eine Konstante f"ur jede Politik pi (f"ur die gleichen \(X_{j}\) und wir setzen \begin{displaymath}c = \sum_{j=0}^{n} E(X_{j})\end{displaymath}:

\begin{displaymath}E_{\pi}(M) = \frac{E_{\pi}(D) + c}{2}\end{displaymath}

Also ergibt sich nun auch formal, dass durch Minimierung der erwarteten Differenz D in der einer der Pr"ozessoren unbesch"aftigt ist, auch die Makespan minimiert wird.


Als n"achstes zeige ich, dass eine Politik die in aufsteigender Reihenfolge der \(u_{i}\)s (also der ~~~ aufsteigend Zeiten) angeordnet ist, optimal ist.
Dies wird wie folgt induktiv bewiesen:

Behauptung: Sei eine Politik \begin{displaymath}\pi = (0,2,1,3,\ldots,n)\end{displaymath} und eine Politik \begin{displaymath}\bar \pi` = (0, 1,2,3,\ldots,n)\end{displaymath} gegeben. Ist \begin{displaymath}u_{1} = min k \ge 1 u_{k}\end{displaymath} dann gilt \begin{displaymath}E_{\bar \pi`}(D)\le = E_{\pi}(D)\end{displaymath}.

Die Idee ist also, eine einzelne Transposition durchzuf"uhren und dann zu beweisen, dass sich dadurch \(E_{\pi}\)(D) erh"oht.

Beweis:

Seien \(\pi_{j}\) und \(\bar\pi_{j}\), j \(\ge\) 0 die zu \(\pi\) und \(\bar\pi\) geh"origen Wahrscheinlichkeiten, dass die letzte von den Prozessoren abgeschlossene Aufgabe Job j ist, also z.B. Prozessor A fertig ist und Prozessor B noch an Aufgabe j arbeitet und keine weiteren Aufgaben mehr in der Warteschlange/Politik liegen. %~~~

Klar ist \begin{displaymath}\pi_{0} = \bar\pi_{0} = P(X_{0} > \sum_{j=1}^{n} X_{j})\end{displaymath}, dass also der erste Job der Wahrscheinlichkeit entspricht, dass \(X_{0}\) gr"osser als alle anderen zusammengenommen ~~ ergibt, da \(\pi_{0}\) ja jeweils der erste Job in beiden Politiken ist.

Wir wollen nun durch Induktion zeigen, dass \begin{displaymath}\bar\pi_{1} \le \pi_{1}, \bar\pi_{j} \ge \pi_{j}\end{displaymath} f"ur \begin{displaymath}j=2,\ldots,n\end{displaymath}.

Induktionsanfang: F"ur n = 1 f"allt \begin{displaymath}\bar\pi_{j} \ge \pi_{j}\end{displaymath} f"ur j = 2,\ldots,n sowieso weg, da n < 2 und \begin{displaymath}\bar\pi_{1} \le \pi_{1}\end{displaymath} gilt nach der Voraussetzung \(u_{1}\) = min \(u_{k}\), da \(\pi\) an der Stelle 1 \(u_{1}\) stehen hat und \(\bar\pi\) dagegen \(u_{2}\).

Induktionsvoraussetzung: Es gelte also die Behauptung f"ur den Fall, dass nur n-1 jobs (zus"atzlich zu job 0) vorliegen.

Induktionsschluss: Seien nun n jobs zu erledigen und seien \(\Phi_{j}\) und \(\bar\Phi_{j}\), wieder zugeh"orig zu \(\pi\) und \(\bar\pi\), j=1,\ldots,n-1 die Wahrscheinlichkeiten, dass Job j der letzte der Jobs 0,\ldots,n-1.

%Exponential and the "Lack of Memory" property 	Here the "Lack of Memory" property of the exponential is used. See page 198-199 and equation 4-12. Essentially, this property says that the P(X>t) d"os not depend on how much time has already elapsed. For example, if the average life of a light bulb is 2 years (exponentially distributed), then the probability that the bulb will last 1 more year d"os not depend on how old the bulb already is.

%~~ Dadurch, und das bei beiden Politiken der letzte Job n ist, erhalten wir:

\begin{displaymath}\pi_j = \Phi_j u_n / (u_n + u_j)\end{displaymath} und \begin{displaymath}\bar\pi_j = \bar\Phi_j u_n / (u_n + u_j), j = 1...n-1\end{displaymath}


Woher kommt dieses \begin{displaymath}\frac{u_{n}}{u_{n}+u_{j}}\end{displaymath}?
%Das frage ich mich auch :-(

Also, was ist \(u_{n}\)? u war die Lambda der Exponentialverteilung, also \begin{displaymath}0 \le u \le 1\end{displaymath} und \begin{displaymath}X_{j} = F(t) = 1 - e^{-u_{j}t}\end{displaymath}.
Aus \(0 \le u\) folgt, dass der Quotient \(<1\) ist, die Wahrscheinlichkeit also kleiner ist, als \(\Phi_{j}\). 
Nun, wie haben wir \(\Phi_{j}\) definiert? \(\Phi_{j}\) war die Wahrscheinlichkeit, dass j der letzte Job ist, wenn der n-te Job weggelassen wird, also im Fall mit \((n-1)\) Jobs. Legen wir nun einfach noch den n-ten Job hinzu, ergeben sich logischerweise 2 F"alle: j bleibt der letzte Job oder n wird der letzte Job. Da dies auf den ersten Blick nicht einleuchtend ist, hier wieder ein Bild:

%|||||||||| (n-4) ||||||||| (n-2) ||| (n-1) ||||||||||| (n)
%|||||||||||||||||| (n-3) ||||||||||||||||||| (j = n-2)

\(\pi_{j}\) war definiert als \begin{displaymath}P(X_{j} > \sum_{i=1}^{n} X_{i} - X_{j}\end{displaymath}
\(\Phi_{j}\) war definiert als \begin{displaymath}P(X_{j} > [\sum_{i=1}^{n-1} X_{i}] - X_{j})) = P(X_{j} > [\sum_{i=1}^{n-1} X_{i}] - X_{j} + X_{n}) * \frac{u_{n} + u_{j}}{u_{n}}\end{displaymath}

%HIER NOCHMAL NACHFRAGEN!

\begin{displaymath}\Rightarrow \pi_{j} =  \Phi_{j} * \frac{u_{n}}{u_{n} + u_{j}}\end{displaymath}

und \begin{displaymath}\bar\pi_{j} = \bar\Phi_{j} * \frac{u_{n}}{u_{n} + u_{j}}\end{displaymath}

\begin{displaymath}\Rightarrow \bar\pi_{1} \le \pi_{1}, \bar\pi_{j} \ge \pi_{j}, j=2,\ldots,n-1\end{displaymath}


Kennen wir alle Wahrscheinlichkeiten \(\pi_{0}\) bis \(\pi_{n-1}\) kennen wir auch \(\pi_{n}\), da egal welche Politik wir w"ahlen AUF JEDEN FALL eine Politik die letzte ist.

Wir benutzen einfach die Gegenwahrscheinlichkeit:

\begin{displaymath}\pi_{n} = 1 - \sum_{j=0}^{n} \pi_{j}\end{displaymath}

bzw. \begin{displaymath}\bar\pi_n - \pi_n =  \sum(\bar\pi_j - \pi_j) = \sum((\Phi_{j} - \bar\Phi_{j}) * \frac{u_{n}}{u_{n} + u_{j}} =\end{displaymath}
\begin{displaymath}(\bar\Phi_{1} - \Phi_{1}) \frac{u_{1}}{u_{1} + u_{n}} + \sum_{j=2}^{n-1} (\bar\Phi_{j} - \Phi_{j}) \frac{u_{j}}{u_{j}+u_{n}} \end{displaymath}
%~~` ja ok
%~~

Wir haben jetzt also per Induktion gezeigt, dass der Erwartungswert der Zeit in der ein Prozessor still steht bei Politik (0,2,1,3,\ldots,n) mindestens so gro"s ist wie bei (0,1,2,3,\ldots,n), falls \begin{displaymath}u_{1} = min k u_{k}\end{displaymath}

Mit dieser Grundlage k"onnen wir nun zus"atzlich zeigen, dass \((0,1,2,\ldots,n)\) die optimale Politik ist um den Erwartungswert f"ur D zu minimieren.

Also:
Sei wieder \begin{displaymath}u_{1} \le u_{2} \le \ldots \le u_{n}\end{displaymath} (*)

Nehmen wir also eine neue Politik her, die nicht mit (0,1,...) beginnt, dann k"onnen wir mit dem bewiesenen Satz 3.1~~~~ direkt folgern, dass diese neue Politik mehr Zeit ben"otigt als die alte. Wenn wir jetzt noch zeigen k"onnten, dass wenn Job 1 an einer beliebigen anderen Stelle k steht, die Politik besser w"are bei der die Stellen k und k-1 vertauscht sind, h"atten wir die Behauptung bewiesen:

Seien also \(\pi\) und \(\bar\pi\) zwei Politiken mit der selben Reihenfolge der Jobs, wobei bei \(\pi\) an Stelle k eine 1 steht und bei \(\bar\pi\) an Stelle k-1 eine 1 steht und \(i_{k}\) von \(\bar\pi\) = \(i_{k-1}\) von \(\pi\) ist.

\begin{displaymath}\pi  = (0,i_{1},i_{2},\ldots,i_{k-2},i_{k-1},i_{k} = 1,i_{k+1},\ldots,i_{n})\end{displaymath}
\begin{displaymath}\bar\pi = (0, i_{1}, i_{2},\ldots, i_{k-2}, i_{k-1} = 1, i_{k}, i_{k+1},\ldots, i_{n})\end{displaymath}

Wieder nach der Eigenschaft des \textsc{lack of memory} haben die Jobs 0 bis \(i_{k-2}\) keinen Einfluss auf die Nachfolgenden, wie auch \(i_{k-1}\) und \(i_{k}\) keinen Einfluss auf die Zeiten der nachfolgenden Jobs haben. Somit kann man diese zu einem neuen Job 0 zusammenfassen und es ergibt sich:

\begin{displaymath}\pi = (0, i_{k-1}, i_{k} = 1,\ldots), \bar\pi = (0, i_{k-1} = 1, i_{k},\ldots)\end{displaymath}
bzw.
\begin{displaymath}\pi = (0, x, 1,\ldots), \bar\pi = (0, 1, x,\ldots)\end{displaymath}

In 3.1 ~~~ haben wir nur (0,2,1,3,\ldots) mit (0,1,2,3,\ldots) verglichen. Es ist aber nicht entscheidend, ob beim Beweis die zweite Stelle 2 oder irgend eine andere Zahl j>1 ist, sondern nur, dass \begin{displaymath}u_{j} \le u_{1}\end{displaymath} gilt. Da hier \(x>1\), also \(j>1\) ist, folgt nach Voraussetzung (*), dass \begin{displaymath}u_{1} \le u_{j}\end{displaymath} und somit ist Lemma 3.1 anwendbar.

Das Spielchen kann nun mit 2, 3 usw. wiederholt werden, bis alle Jobs in aufsteigender Reihenfolge da stehen woraus die Behauptung folgt und bewiesen ist, dass eine aufsteigende Jobanordnung die Aufgabenstellung l"ost. Zu Betonen ist hierbei, dass man dies, wie gesagt, in aufsteigender Reihenfolge durchf"uhrt, da ein solches Vertauschen eines niederwertigen durch ein h"oherwertigen nicht grunds"atzlich von Vorteil ist, denn es muss nicht nur \begin{displaymath}u_{j} \le u_{k}\end{displaymath} sein, \(u_{j}\) muss auch das kleinste Element, abgesehen von \(u_{j-1}\), \(u_{j-2}\) etc. die ja schon nach vorne geschoben wurden, aller Jobs sein, also \begin{displaymath}u_{j} \le min u_{j+1},\ldots,u_{n}\end{displaymath}%~~~~) sein.~~~~ //TODO

Das w"are es eigentlich fast schon zum zweiten Punkt dieses Vortrags. Ich m"ochte jedoch noch einmal kurz auf die anfangs erw"ahnten Politiken eingehen, die man durch geschicktes W"ahlen der Form der Politik und der Einf"ugeregel wegfallen hat lassen.

\newpage
\section{Ein Prozessor mit begrenzter Zeit}

Mit diesem Problembeispiel m"ochte ich auf einen weiteren Aspekt des Stochastic Scheduling eingehen. Wir haben wie vorher auch n Jobs die ausgef"uhrt werden sollen, haben diesmal aber eine feste Zeitbegrenzung t, die wahrscheinlich dazu f"uhrt, dass nicht alle Jobs ausgef"uhrt werden k"onnen.
Der Unterschied zum ersten Beispiel ist nun aber, dass wir jedem Job i eine Priorit"at \(R_{i}\) zuordnen und die Qualit"at einer Politik nicht wie vorher "uber die Zeit sondern "uber die Summe der Priorit"aten bestimmen, es soll also \begin{displaymath}\sum_{i=0}^{n}R_{i}s_{i}\end{displaymath} maximiert werden, wobei \(s_{i} = 1\) ist, wenn der Job ausgef"uhrt werden konnte und 0 wenn nicht.
Auch hier gilt wieder die 'lack-of-memory' Eigenschaft, fr"uher ausgef"uhrte Jobs haben also keine Wirkung auf die ben"otigte Zeit von nachfolgenden Jobs.

Das \(s_{i}\) kann man nat"urlich auch als Wahrscheinlichkeit schreiben, wenn man den Erwartungswert der Qualit"at der Politik haben will: ~~~

....


%"Lack of memory" means the probability of failure in a specific time interval is the same regardless of the starting point of that time interval %TODO
%"ged"achtnislos"

%Interpretation: Ein Objekt lebt bereits mehr als x
%Zeiteinheiten. Dann ist die bedingte Wkt. dafur, ¨
%dass es innerhalb der n¨  achsten x Zeiteinheiten
%ausf¨ lt, gleich der unbedingten Wkt., dass es uber-
%    al                                         ¨
%haupt h¨ ochstens x Zeiteinheiten lebt. Mit anderen
%Worten, das Objekt ist stets (so gut wie) neu" .
                              "






\newpage
\section{Zwei Prozessoren mit begrenzter Zeit}

Im letzten Teil kombinieren wir nun die vorherigen 2 Problemstellungen. Wir haben 2 Prozessoren, eine feste Zeit t und f"ur jeden Job eine Priorit"at \(R_{j}\).
Nach 2.1 ist der Gesamtgewinn ~~ unter einer Politik \(\pi\):
\(E_{\pi}\)(Gesamtgewinn abh"angig von t) = \begin{displaymath}\sum_{j=0}^{n} u_{j}R_{j}E_{\pi}\end{displaymath}(Verarbeitungszeit von Job j nach t ~~)

Es sieht nun so aus, als ob wir hier wie in 2.1, der Fall bei dem wir nur ein Prozessor zur Verf"ugung hatten, die optimale Politik eine absteigende Reihenfolge der \(u_{j}\)\(R_{j}\) ist, also wir Jobs um so weiter hinten positionieren je gr"o"ser die erwartete ben"otigte Zeit und je kleiner die Priorit"at ist.

Dies ist aber nicht der Fall wie ein kleines Gegenbeispiel zeigt:

\(u_{j}\)\(R_{j}\) == 1 f"ur alle j und \begin{displaymath}u_{1} < u_{2} < \cdots < u_{n}\end{displaymath}.

Es w"urde folgen, dass, da ja alle \(u_{j}\)\(R_{j}\) gleich gro"s sind, jede beliebige Reihenfolge optimal ist. Dies stimmt auch im Fall von einem Prozessor, bei zwei Prozessoren tritt jedoch wie anfangs besprochen eine immer gr"o"sere Idle Zeit auf, wenn wir die Jobs absteigend nach den \(u_{j}\)s (bzw. aufsteigend nach den ben"otigten Zeiten) sortieren.

Wir k"onnen jedoch zeigen, dass wie im Fall ohne Priorit"aten eine aufsteigende Reihenfolge optimal ist, wenn wir gewisse Bedingungen voraussetzen.

Behauptung:
Sei 
\begin{displaymath}u_{1} \le u_{2} \le \cdots \le u_{n}\end{displaymath} 
und 
\begin{displaymath}u_{1}R_{1} \ge u_{2}R_{2} \ge \cdots \ge u_{n}R_{n} \ge 0\end{displaymath}
dann ist die Reihenfolge (1, 2, \ldots, n) optimal und maximiert den zu erwartenden Gesamtgewinn f"ur Zeit \(t>0\).

Beweis:


\newpage
\section{Weitere verwandte Probleme und Anwendungen}
Schluss:

Man kann sich eine Vielzahl von weiteren Problemen der Art denken:
Zwei Prozessoren hintereinander, zweimal zwei Prozessoren hintereinander parallelgeschaltet, Aenderung der Prozessorgeschwindigkeit w"ahrend des Verlaufs, Ausfall von Prozessoren usw. usw.

\newpage
\section{Zusammenfassung}

\begin{thebibliography}{99}
\bibitem{Ross} {\sc Sheldon M. Ross:}  \textit{Introduction to stochastic dynamic programming - Probabilityand mathematical statistics}, First edition, Academic Press, Inc., 1983
\end{thebibliography}
\end{document}


